{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Fundamentos de Autograd\n",
    "\n",
    "La característica Autograd de PyTorch es parte de lo que hace a PyTorch flexible y rápido para construir proyectos de aprendizaje automático. Permite el cálculo rápido y sencillo de múltiples derivadas parciales (también llamadas gradientes) sobre un cálculo complejo. Esta operación es fundamental para el aprendizaje de redes neuronales basado en la retropropagación.\n",
    "\n",
    "El poder de autograd viene del hecho de que traza su computación dinámicamente en tiempo de ejecución, lo que significa que si su modelo tiene ramas de decisión, o bucles cuyas longitudes no se conocen hasta el tiempo de ejecución, la computación seguirá siendo trazada correctamente, y obtendrá gradientes correctos para impulsar el aprendizaje. Esto, combinado con el hecho de que sus modelos se construyen en Python, ofrece mucha más flexibilidad que los marcos que se basan en el análisis estático de un modelo más rígidamente estructurado para calcular los gradientes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Para qué necesitamos Autograd?\n",
    "\n",
    "\n",
    "Un modelo de aprendizaje automático es una *función*, con entradas y salidas. Para esta discusión, trataremos las entradas como un vector *i*-dimensional $\\vec{x}$, con elementos $x_{i}$. Entonces podemos expresar el modelo, *M*, como una función de valor vectorial de la entrada: $\\vec{y} = \\vec{M}(\\vec{x})$. (Tratamos el valor de la salida de M como un vector porque, en general, un modelo puede tener cualquier número de salidas).\n",
    "\n",
    "Como hablaremos de autograd en el contexto del entrenamiento, nuestra salida de interés será la pérdida del modelo. La *función de pérdida* L($\\vec{y}$) = L($\\vec{M}$ ($\\vec{x}$)) es una función escalar de un solo valor de la salida del modelo. Esta función expresa cuánto se alejó la predicción de nuestro modelo de la salida *ideal* de una entrada particular. *Nota: Después de este punto, a menudo omitiremos el signo signo de vector donde debe ser contextualmente claro - por ejemplo,* $y$ en lugar de $\\vec y$.\n",
    "\n",
    "En el entrenamiento de un modelo, queremos minimizar la pérdida. En el caso idealizado de un modelo perfecto, eso significa ajustar sus pesos de aprendizaje -es decir, los parámetros ajustables de la función- de forma que la pérdida sea cero para todas las entradas. En el mundo real, significa un proceso iterativo de ajuste de los pesos de aprendizaje hasta que veamos que obtenemos una pérdida tolerable para una amplia variedad de entradas.\n",
    "\n",
    "¿Cómo decidimos hasta dónde y en qué dirección modificar los pesos? Queremos *minimizar* la pérdida, lo que significa hacer su primera derivada con respecto a la entrada igual a 0: $\\frac{\\partial L}{\\partial x} = 0$.\n",
    "\n",
    "Recordemos, sin embargo, que la pérdida no es *directamente* derivada de la entrada, sino una función de la salida del modelo (que es una función de la entrada directamente), $\\frac{\\partial L}{\\partial x}$ =\n",
    "$\\frac{\\partial {L({\\vec y})}}{\\partial x}$. Por la regla de la cadena del cálculo diferencial, tenemos $\\frac{\\partial {L({\\vec y})}}{\\partial x}$ =\n",
    "$\\frac{\\partial L}{\\partial y}\\frac{\\partial y}{\\partial x}$ =\n",
    "$\\frac{\\partial L}{\\partial y}\\frac{\\partial M(x)}{\\partial x}$.\n",
    "\n",
    "$\\frac{\\partial M(x)}{\\partial x}$es donde las cosas se ponen complejas. Las derivadas parciales de las salidas del modelo con respecto a su entradas, si tuviéramos que ampliar la expresión utilizando la regla de la cadena de nuevo, implicaría muchas derivadas parciales locales sobre cada peso de aprendizaje cada peso de aprendizaje multiplicado, cada función de activación y cualquier otra en el modelo. La expresión completa para cada una de estas derivada parcial es la suma de los productos del gradiente local de *cada camino posible posible* a través del gráfico de cálculo que termina con la variable cuyo gradiente intentamos medir.\n",
    "\n",
    "En particular, nos interesan los gradientes sobre los pesos de aprendizaje nos dicen *en qué dirección cambiar cada peso* para que la función de pérdida se acerque a cero. para que la función de pérdida se acerque a cero.\n",
    "\n",
    "Dado que el número de tales derivadas locales (cada una correspondiente a un de cálculo del modelo) tenderá a aumentar exponencialmente con la profundidad de un modelo neuronal. exponencialmente con la profundidad de una red neuronal, también lo hace la complejidad para calcularlas. Aquí es donde entra en juego autograd: Rastrea el historial de cada cálculo. Cada tensor computado en tu modelo PyTorch lleva un historial de sus tensores de entrada y la función usada para crearlo. Combinado con el hecho de que las funciones PyTorch destinadas a actuar sobre tienen cada una una implementación incorporada para calcular sus propias derivadas, esto acelera enormemente el cálculo de las derivadas locales necesarias para el aprendizaje. derivadas locales necesarias para el aprendizaje. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un ejemplo sencillo\n",
    "\n",
    "Esto ha sido mucha teoría, pero ¿cómo se utiliza autograd en la práctica?\n",
    "\n",
    "Empecemos con un ejemplo sencillo. Primero, haremos algunas importaciones\n",
    "para poder representar gráficamente nuestros resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, vamos a crear un tensor de entrada lleno de valores espaciados uniformemente en el intervalo $[0, 2{\\pi}]$, y especificar ``requires_grad=True``. (Como la mayoría de las funciones que crean tensores, ``torch.linspace()`` acepta una opción ``requires_grad``). Establecer esta bandera significa que en cada cálculo que siga, autograd irá acumulando el historia del cálculo en los tensores de salida de ese cálculo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.2618, 0.5236, 0.7854, 1.0472, 1.3090, 1.5708, 1.8326, 2.0944,\n",
      "        2.3562, 2.6180, 2.8798, 3.1416, 3.4034, 3.6652, 3.9270, 4.1888, 4.4506,\n",
      "        4.7124, 4.9742, 5.2360, 5.4978, 5.7596, 6.0214, 6.2832],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "a = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, realizaremos un cálculo y graficaremos su resultado en función de sus entradas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b6dcdca0a0>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "b = torch.sin(a)\n",
    "plt.plot(a.detach(), b.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Echemos un vistazo más de cerca al tensor ``b``. Cuando lo imprimimos, vemos un indicador de que está rastreando su historia de cálculo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0000e+00,  2.5882e-01,  5.0000e-01,  7.0711e-01,  8.6603e-01,\n",
      "         9.6593e-01,  1.0000e+00,  9.6593e-01,  8.6603e-01,  7.0711e-01,\n",
      "         5.0000e-01,  2.5882e-01, -8.7423e-08, -2.5882e-01, -5.0000e-01,\n",
      "        -7.0711e-01, -8.6603e-01, -9.6593e-01, -1.0000e+00, -9.6593e-01,\n",
      "        -8.6603e-01, -7.0711e-01, -5.0000e-01, -2.5882e-01,  1.7485e-07],\n",
      "       grad_fn=<SinBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Este ``grad_fn`` nos da una pista de que cuando se ejecuta el paso de retropropagación(backpropagation) y calcular gradientes, tendremos que calcular el derivada de $\\sin(x)$ para todas las entradas de este tensor.\n",
    "\n",
    "Vamos a realizar algunos cálculos más:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0000e+00,  5.1764e-01,  1.0000e+00,  1.4142e+00,  1.7321e+00,\n",
      "         1.9319e+00,  2.0000e+00,  1.9319e+00,  1.7321e+00,  1.4142e+00,\n",
      "         1.0000e+00,  5.1764e-01, -1.7485e-07, -5.1764e-01, -1.0000e+00,\n",
      "        -1.4142e+00, -1.7321e+00, -1.9319e+00, -2.0000e+00, -1.9319e+00,\n",
      "        -1.7321e+00, -1.4142e+00, -1.0000e+00, -5.1764e-01,  3.4969e-07],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([ 1.0000e+00,  1.5176e+00,  2.0000e+00,  2.4142e+00,  2.7321e+00,\n",
      "         2.9319e+00,  3.0000e+00,  2.9319e+00,  2.7321e+00,  2.4142e+00,\n",
      "         2.0000e+00,  1.5176e+00,  1.0000e+00,  4.8236e-01, -3.5763e-07,\n",
      "        -4.1421e-01, -7.3205e-01, -9.3185e-01, -1.0000e+00, -9.3185e-01,\n",
      "        -7.3205e-01, -4.1421e-01,  4.7684e-07,  4.8236e-01,  1.0000e+00],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "c = 2 * b\n",
    "print(c)\n",
    "\n",
    "d = c + 1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, calculemos una salida de un solo elemento. Cuando se llama a ``.backward()`` en un tensor sin argumentos, se espera que el tensor de llamada contenga sólo un elemento único, como ocurre cuando se calcula una función de pérdida.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = d.sum()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada ``grad_fn`` almacenada con nuestros tensores te permite recorrer el cálculo hasta sus entradas con su propiedad ``next_functions``. Podemos ver a continuación que la exploración de esta propiedad en ``d`` nos muestra las funciones de gradiente para todos los tensores anteriores. Observa que ``a.grad_fn`` aparece como ``None``, indicando que era una entrada a la función sin historia propia.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\n",
      "<AddBackward0 object at 0x000001B6DEECBD00>\n",
      "((<MulBackward0 object at 0x000001B6DEECBF40>, 0), (None, 0))\n",
      "((<SinBackward object at 0x000001B6DEECBD00>, 0), (None, 0))\n",
      "((<AccumulateGrad object at 0x000001B6BF7E9340>, 0),)\n",
      "()\n",
      "\n",
      "c:\n",
      "<MulBackward0 object at 0x000001B6DEECA130>\n",
      "\n",
      "b:\n",
      "<SinBackward object at 0x000001B6DEECA130>\n",
      "\n",
      "a:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('d:')\n",
    "print(d.grad_fn)\n",
    "print(d.grad_fn.next_functions)\n",
    "print(d.grad_fn.next_functions[0][0].next_functions)\n",
    "print(d.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)\n",
    "print(d.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions)\n",
    "print('\\nc:')\n",
    "print(c.grad_fn)\n",
    "print('\\nb:')\n",
    "print(b.grad_fn)\n",
    "print('\\na:')\n",
    "print(a.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con toda esta maquinaria en marcha, ¿cómo sacamos las derivadas? Llamas al método ``backward()`` en la salida, y compruebas la propiedad ``grad`` de la entrada para inspeccionar los gradientes:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.0000e+00,  1.9319e+00,  1.7321e+00,  1.4142e+00,  1.0000e+00,\n",
      "         5.1764e-01, -8.7423e-08, -5.1764e-01, -1.0000e+00, -1.4142e+00,\n",
      "        -1.7321e+00, -1.9319e+00, -2.0000e+00, -1.9319e+00, -1.7321e+00,\n",
      "        -1.4142e+00, -1.0000e+00, -5.1764e-01,  2.3850e-08,  5.1764e-01,\n",
      "         1.0000e+00,  1.4142e+00,  1.7321e+00,  1.9319e+00,  2.0000e+00])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b6deeeea60>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "out.backward()\n",
    "print(a.grad)\n",
    "plt.plot(a.detach(), a.grad.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activar y desactivar Autograd\n",
    "\n",
    "Hay situaciones en las que necesitará un control preciso sobre la activación del autograd. Hay varias maneras de hacerlo, dependiendo de la situación.\n",
    "\n",
    "La más simple es cambiar la bandera ``requires_grad`` en un tensor directamente:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], requires_grad=True)\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]], grad_fn=<MulBackward0>)\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 3, requires_grad=True)\n",
    "print(a)\n",
    "\n",
    "b1 = 2 * a\n",
    "print(b1)\n",
    "\n",
    "a.requires_grad = False\n",
    "b2 = 2 * a\n",
    "print(b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la celda anterior, vemos que b1 tiene un ``grad_fn`` (es decir, una historia de cálculo trazada), que es lo que esperamos, ya que se derivó de un tensor, ``a``, que tenía autograd activado. Cuando desactivamos ``autograd`` explícitamente con ``a.requires_grad = False``, la historia del cálculo ya no se rastrea, como vemos cuando calculamos ``b2``.\n",
    "\n",
    "Si sólo necesita ``autograd`` apagado temporalmente, una mejor manera es utilizar la ``torch.no_grad():``\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]], grad_fn=<AddBackward0>)\n",
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]])\n",
      "tensor([[6., 6., 6.],\n",
      "        [6., 6., 6.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 3, requires_grad=True) * 2\n",
    "b = torch.ones(2, 3, requires_grad=True) * 3\n",
    "\n",
    "c1 = a + b\n",
    "print(c1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    c2 = a + b\n",
    "\n",
    "print(c2)\n",
    "\n",
    "c3 = a * b\n",
    "print(c3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``torch.no_grad()`` también puede utilizarse como decorador de funciones o métodos:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]], grad_fn=<AddBackward0>)\n",
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "def add_tensors1(x, y):\n",
    "    return x + y\n",
    "\n",
    "@torch.no_grad()\n",
    "def add_tensors2(x, y):\n",
    "    return x + y\n",
    "\n",
    "\n",
    "a = torch.ones(2, 3, requires_grad=True) * 2\n",
    "b = torch.ones(2, 3, requires_grad=True) * 3\n",
    "\n",
    "c1 = add_tensors1(a, b)\n",
    "print(c1)\n",
    "\n",
    "c2 = add_tensors2(a, b)\n",
    "print(c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay un gestor de contexto correspondiente, ``torch.enable_grad()``, para activar autograd cuando aún no lo está. También puede usarse como decorador.\n",
    "\n",
    "Finalmente, puedes tener un ``tensor`` que requiera seguimiento de gradiente, pero quieres una copia que no lo requiera. Para esto tenemos el método ``detach()`` del objeto Tensor - crea una copia del tensor que se separa de la historia del cálculo:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4529, 0.1175, 0.6566, 0.4978, 0.5896], requires_grad=True)\n",
      "tensor([0.4529, 0.1175, 0.6566, 0.4978, 0.5896])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, requires_grad=True)\n",
    "y = x.detach()\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hicimos esto arriba cuando quisimos graficar algunos de nuestros tensores. Esto es porque ``matplotlib`` espera un ``array NumPy`` como entrada, y la conversión implícita de un ``tensor PyTorch`` a un ``array NumPy`` no está habilitada para tensores con ``requires_grad=True``. Hacer una copia separada nos permite seguir adelante.\n",
    "\n",
    "\n",
    "## Autograd y Operaciones In-place\n",
    "\n",
    "En todos los ejemplos de este cuaderno hasta ahora, hemos utilizado variables para capturar los valores intermedios de un cálculo. ``Autograd`` necesita estos valores intermedios para realizar cálculos de gradiente. Por esta razón, debes tener cuidado con el uso de operaciones ``in-place`` cuando utilices ``autograd``. Una operación In-place es una operación que cambia directamente el contenido de un tensor dado sin hacer una copia. Las operaciones inplace en pytorch siempre tienen un postfijo _, como .add_() o .scatter_(). Las operaciones de Python como += o *= también son operaciones in situ. Hacerlo puede destruir la información que necesitas para calcular las derivadas en el ``backward()``. ``PyTorch`` incluso te detendrá si intentas una operación in-place en una variable de hoja que requiere autograd, como se muestra a continuación.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a leaf Variable that requires grad is being used in an in-place operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-4f751c48411e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2.\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msin_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "source": [
    "a = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\n",
    "torch.sin_(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd Profiler\n",
    "\n",
    "``Autograd`` rastrea cada paso de su cálculo en detalle. Tal historial de cómputo, combinado con la información de tiempo, sería un generador de perfiles útil, y autograd tiene esa función integrada. Aquí hay un ejemplo rápido de uso:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "           Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "---------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "      aten::div        48.49%      36.122ms        50.69%      37.755ms      37.755us      38.639ms        50.92%      38.639ms      38.639us          1000  \n",
      "      aten::mul        47.22%      35.175ms        49.31%      36.733ms      36.733us      37.246ms        49.08%      37.246ms      37.246us          1000  \n",
      "    aten::empty         4.28%       3.192ms         4.28%       3.192ms       1.596us       0.000us         0.00%       0.000us       0.000us          2000  \n",
      "---------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 74.489ms\n",
      "Self CUDA time total: 75.885ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "run_on_gpu = False\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    run_on_gpu = True\n",
    "    \n",
    "x = torch.randn(2, 3, requires_grad=True)\n",
    "y = torch.rand(2, 3, requires_grad=True)\n",
    "z = torch.ones(2, 3, requires_grad=True)\n",
    "\n",
    "with torch.autograd.profiler.profile(use_cuda=run_on_gpu) as prf:\n",
    "    for _ in range(1000):\n",
    "        z = (z / x) * y\n",
    "        \n",
    "print(prf.key_averages().table(sort_by='self_cpu_time_total'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El generador de perfiles también puede etiquetar subbloques individuales de código, desglosar\n",
    "datos por forma de tensor de entrada y exportar datos como herramientas de seguimiento de Chrome\n",
    "archivo. Para obtener detalles completos de la API, consulte el\n",
    "[documentación](https://pytorch.org/docs/stable/autograd.html#profiler)_.\n",
    "\n",
    "## Tema avanzado: más detalles de Autograd y la API de alto nivel\n",
    "\n",
    "Si tiene una función con una entrada n-dimensional y m-dimensional\n",
    "salida, $\\vec{y}=f(\\vec{x})$, el gradiente completo es una matriz de\n",
    "la derivada de cada salida con respecto a cada entrada, llamada\n",
    "*jacobiano:*\n",
    "\n",
    "\\begin{align}J\n",
    "     =\n",
    "     \\left(\\begin{array}{ccc}\n",
    "     \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    "     \\vdots & \\ddots & \\vdots\\\\\n",
    "     \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "     \\end{array}\\right)\\end{align}\n",
    "\n",
    "Si tiene una segunda función, $l=g\\left(\\vec{y}\\right)$ que\n",
    "toma una entrada m-dimensional (es decir, la misma dimensionalidad que la\n",
    "salida anterior) y devuelve una salida escalar, puede expresar sus\n",
    "gradientes con respecto a $\\vec{y}$ como vector columna,\n",
    "$v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}$\n",
    "- que en realidad es solo un jacobiano de una columna.\n",
    "\n",
    "Más concretamente, imagine la primera función como su modelo PyTorch (con potencialmente muchas entradas y muchas salidas) y la segunda función como una función de pérdida (con la salida del modelo como entrada y el valor de pérdida como salida escalar).\n",
    "\n",
    "Si multiplicamos el jacobiano de la primera función por el gradiente de la segunda función y aplicamos la regla de la cadena, obtenemos:\n",
    "\n",
    "\n",
    "\\begin{align}J^{T}\\cdot v=\\left(\\begin{array}{ccc}\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n",
    "   \\vdots & \\ddots & \\vdots\\\\\n",
    "   \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "   \\end{array}\\right)\\left(\\begin{array}{c}\n",
    "   \\frac{\\partial l}{\\partial y_{1}}\\\\\n",
    "   \\vdots\\\\\n",
    "   \\frac{\\partial l}{\\partial y_{m}}\n",
    "   \\end{array}\\right)=\\left(\\begin{array}{c}\n",
    "   \\frac{\\partial l}{\\partial x_{1}}\\\\\n",
    "   \\vdots\\\\\n",
    "   \\frac{\\partial l}{\\partial x_{n}}\n",
    "   \\end{array}\\right)\\end{align}\n",
    "\n",
    "Nota: también podría usar la operación equivalente $v^{T}\\cdot J$,\n",
    "y recuperar un vector de fila.\n",
    "\n",
    "El vector columna resultante es el *gradiente de la segunda función con\n",
    "respecto a las entradas del primero* - o en el caso de nuestro modelo y\n",
    "función de pérdida, el gradiente de la pérdida con respecto al modelo\n",
    "entradas.\n",
    "\n",
    "**``torch.autograd`` es un motor para computar estos productos.** Este\n",
    "es cómo acumulamos los gradientes sobre los pesos de aprendizaje durante el\n",
    "pase hacia atrás.\n",
    "\n",
    "Por esta razón, la llamada ``hacia atrás()`` puede *también* tomar una opción\n",
    "entrada vectorial. Este vector representa un conjunto de gradientes sobre el tensor,\n",
    "que se multiplican por el jacobiano del tensor trazado por autograd que\n",
    "lo precede. Probemos un ejemplo específico con un vector pequeño:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -748.8833, -1573.7837,  -336.0165], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si intentáramos llamar a ``y.backward()`` ahora, obtendríamos un error de tiempo de ejecución y un\n",
    "mensaje de que los gradientes solo se pueden calcular *implícitamente* para escalar\n",
    "salidas. Para una salida multidimensional, autograd espera que proporcionemos\n",
    "gradientes para esas tres salidas que puede multiplicar en el\n",
    "jacobiano:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float) # stand-in for gradients\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Tenga en cuenta que los gradientes de salida están todos relacionados con potencias de dos, que\n",
    "esperaríamos de una operación de duplicación repetida.)\n",
    "\n",
    "### La API de alto nivel\n",
    "\n",
    "Hay una API en autograd que le brinda acceso directo a importantes\n",
    "operaciones diferenciales con matrices y vectores. En particular, le permite\n",
    "para calcular las matrices jacobiana y *hessiana* de una determinada\n",
    "función para entradas particulares. (El hessiano es como el jacobiano, pero\n",
    "expresa todas las derivadas *segunda* parciales.) También proporciona métodos\n",
    "para tomar productos vectoriales con estas matrices.\n",
    "\n",
    "Tomemos el jacobiano de una función simple, evaluada para un 2\n",
    "entradas de un solo elemento:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.0204]), tensor([0.7157]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[2.0412]]), tensor([[3.]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def exp_adder(x, y):\n",
    "    return 2 * x.exp() + 3 * y\n",
    "\n",
    "inputs = (torch.rand(1), torch.rand(1)) # arguments for the function\n",
    "print(inputs)\n",
    "torch.autograd.functional.jacobian(exp_adder, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si miras de cerca, la primera salida debería ser igual a $2e^x$ (ya que\n",
    "la derivada de $e^x$ es $e^x$), y el segundo valor\n",
    "debería ser 3.\n",
    "\n",
    "Por supuesto, puede hacer esto con tensores de orden superior:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.4203, 0.8772, 0.5600]), tensor([0.7472, 0.7861, 0.6246]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[3.0449, 0.0000, 0.0000],\n",
       "         [0.0000, 4.8083, 0.0000],\n",
       "         [0.0000, 0.0000, 3.5015]]),\n",
       " tensor([[3., 0., 0.],\n",
       "         [0., 3., 0.],\n",
       "         [0., 0., 3.]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = (torch.rand(3), torch.rand(3)) # arguments for the function\n",
    "print(inputs)\n",
    "torch.autograd.functional.jacobian(exp_adder, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método ``torch.autograd.funcional.hessian()`` funciona de manera idéntica\n",
    "(suponiendo que su función es dos veces diferenciable), pero devuelve una matriz\n",
    "de todas las segundas derivadas.\n",
    "\n",
    "También hay una función para calcular directamente el vector-jacobiano\n",
    "producto, si proporciona el vector:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-947.6455, -182.5196, -661.6850]),\n",
       " tensor([2.0480e+02, 2.0480e+03, 2.0480e-01]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def do_some_doubling(x):\n",
    "    y = x * 2\n",
    "    while y.data.norm() < 1000:\n",
    "        y = y * 2\n",
    "    return y\n",
    "\n",
    "inputs = torch.randn(3)\n",
    "my_gradients = torch.tensor([0.1, 1.0, 0.0001])\n",
    "torch.autograd.functional.vjp(do_some_doubling, inputs, v=my_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método ``torch.autograd.funcional.jvp()`` realiza la misma matriz\n",
    "multiplicación como ``vjp()`` con los operandos invertidos. El ``vhp()``\n",
    "y los métodos ``hvp()`` hacen lo mismo para un producto vectorial-hessiano.\n",
    "\n",
    "Para obtener más información, incluidas las notas de rendimiento en los [docs for the\n",
    "funcional\n",
    "API](https://pytorch.org/docs/stable/autograd.html#function-higher-level-api)_\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
